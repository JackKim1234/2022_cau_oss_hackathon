{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1AosAX9DXOlc"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JackKim1234/2022_cau_oss_hackathon/blob/main/hackathon_team08.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AosAX9DXOlc"
      },
      "source": [
        "# **0. 해커톤 진행 주의사항**\n",
        "\n",
        "**1)  개발 관련 주의사항**\n",
        "*   [1. 초기 환경 설정]은 절대 수정하지 말 것\n",
        "*   모든 구현은 [2. 데이터 전처리] 및 [3.모델 생성]에서만 진행\n",
        "*   [4. 모델 저장]에서 team_name 변수 변경 (예.`team_name = 'team01'`)\n",
        " *    트레이닝 중간에 checkpoint를 활용하여 모델을 저장한 경우에도 파일 이름 양식 통일 필수\n",
        "*   Colab 사용중 실수로 데이터 손실이 발생할 수도 있으니 중간 결과값을 github에 업로드 \n",
        " *    \"런타임 -> 런타임 연결 해제 및 삭제\"은 절대 누르지 말 것 (저장한 모델 데이터가 모두 삭제됨)\n",
        " *    \"런타임 -> 런타임 다시시작\"은 클라우드 스토리지에 저장된 모델은 유지됨\n",
        "*   효율적인 구현 및 테스팅을 위해 GPU 가속 기능 활성화\n",
        " *    \"런타임 -> 런타임 유형변경 -> 하드웨어 가속기 -> GPU 설정\"\n",
        "*   주석을 최대한 자세히 작성\n",
        "*   Keras API 관련하여 [Keras Documentation](https://keras.io/) 참조\n",
        "\n",
        "**2) 제출 관련 주의사항**\n",
        "*  제출물\n",
        " *  소스코드 (hackathon_teamXX.ipynb)\n",
        " *  컴파일된 모델 파일 (model_entire_teamXX.h5)\n",
        " *  모델 발표 자료 \n",
        "* 제출 기한: **오후 6시 (단, 발표자료는 12시)**\n",
        "* 제출 방법: [GitHub README](https://github.com/cauosshackathonta/2022_cau_oss_hackathon/) 참조\n",
        "\n",
        " \n",
        "**3) 평가 관련 주의사항**\n",
        "*  모델 성능 = 테스트 데이터 셋 분류 정확도\n",
        " *  model.evaluate(x_test, y_test)\n",
        "*  제출된 모델들의 테스트 데이터 셋 분류 정확도를 기준으로 수상작 결정\n",
        "*  수상 후보들에 대해서는 소스코드를 기반으로 모델 재검증 \n",
        " \n",
        "**4) 수상 실격 사유**\n",
        "*  유사한 소스코드가 적발될 경우\n",
        "*  Pre-trained 모델을 사용한 경우 (transfer learning 포함)\n",
        "*  소스코드와 제출된 모델이 상이한 경우\n",
        "*  개발 관련 주의사항을 지키지 않은 경우\n",
        " *  예: [초기 환경 설정]을 수정한 경우\n",
        "*  데이터 셋을 변조한 경우\n",
        " *  예. 테스트 데이터 셋을 트레이닝 데이터 셋에 포함하여 모델 생성 \n",
        "*  주석이 소스코드와 맞지 않거나 미비할 경우\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67lwEXhUqys1"
      },
      "source": [
        "# **1. 초기 환경 설정**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ms5PBBJ1qSC6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00c9e082-ba61-4c59-de02-57ecc1f4af21"
      },
      "source": [
        "from __future__ import absolute_import, division, print_function, unicode_literals, unicode_literals\n",
        "\n",
        "# tensorflow와 tf.keras 및 관련 라이브러리 임포트\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow import keras\n",
        "from keras.utils import np_utils\n",
        "from keras import datasets, layers, models\n",
        "\n",
        "# 데이터셋 로드 (MNIST, fashion-MNIST, Kujushiji-MNIST, MNIST_corrupted (test only))\n",
        "train_ds, test_ds = tfds.load('mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "\n",
        "train_ds2, test_ds2 = tfds.load('fashion_mnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds2['label'] += 10;\n",
        "test_ds2['label'] += 10;\n",
        "\n",
        "train_ds3, test_ds3 = tfds.load('kmnist', split=['train', 'test'], shuffle_files=False, batch_size=-1)\n",
        "train_ds3['label'] += 20;\n",
        "test_ds3['label'] += 20;\n",
        "\n",
        "test_ds4 = tfds.load('mnist_corrupted/zigzag', split='test', shuffle_files=False, batch_size=-1)\n",
        "\n",
        "# 데이터셋 병합 (training: 180,000개, test: 40,000개)\n",
        "x_train = np.append(np.append(train_ds['image'], train_ds2['image'], 0), train_ds3['image'], 0);\n",
        "y_train = np.append(np.append(train_ds['label'], train_ds2['label'], 0), train_ds3['label'], 0);\n",
        "\n",
        "x_test = np.append(np.append(np.append(test_ds['image'], test_ds2['image'], 0), test_ds3['image'], 0), test_ds4['image'], 0);\n",
        "y_test = np.append(np.append(np.append(test_ds['label'], test_ds2['label'], 0), test_ds3['label'], 0), test_ds4['label'], 0);\n",
        "\n",
        "# 분류를 위해 클래스 벡터를 바이너리 매트릭스로 변환\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "\n",
        "# 총 클래스 개수: 30, 입력 데이터 구조: (28, 28, 1)\n",
        "num_classes = y_train.shape[1]\n",
        "input_shape = x_train.shape[1:]\n",
        "print(num_classes, input_shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "30 (28, 28, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-YjppJpXBO9"
      },
      "source": [
        "# **2. 데이터 전처리**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ9KWTBP6AI1"
      },
      "source": [
        "# 데이터 전처리 (예: normalization)\n",
        "# 원본 데이터와 전처리 후 데이터를 구분하기 위해, 변수명 x_train_after, x_test_after를 변경하지 말 것\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import optimizers\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D, Input, GaussianNoise, BatchNormalization, Dropout, AveragePooling2D, ZeroPadding2D\n",
        "\n",
        "import time\n",
        "\n",
        "x_train_after = x_train / 255\n",
        "x_test_after = x_test / 255\n",
        "\n",
        "x_train_after = x_train_after.reshape((180000, 28, 28, 1))\n",
        "x_test_after = x_test_after.reshape((40000, 28, 28, 1))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-lo-O1yiFpY"
      },
      "source": [
        "# **3. 모델 생성**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DZP4eRmRqgRp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e4b7b3c-555a-4299-ee15-887f3464a781"
      },
      "source": [
        "# 순차 모델 생성 (가장 기본구조)\n",
        "model = keras.Sequential()\n",
        "\n",
        "# Input layer 28 x 28 x 1\n",
        "model.add(Input((28,28,1)))\n",
        "\n",
        "# Convolution2D , Conv 필터 = (4,4), with activation relu\n",
        "model.add(Conv2D(3, (4, 4),activation='relu'))\n",
        "# ZeroPadding2D\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "# Convolution2D , Conv 필터 = (4,4), with activation relu\n",
        "model.add(Conv2D(64, (4, 4),activation='relu'))\n",
        "# MaxPooling2D\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "\n",
        "# ZeroPadding2D\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "# Convolution2D, with activation relu\n",
        "model.add(Conv2D(128, (4, 4),activation='relu'))\n",
        "# ZeroPadding2D\n",
        "model.add(ZeroPadding2D((1,1)))\n",
        "# Convolution2D, with activation relu\n",
        "model.add(Conv2D(128, (4, 4),activation='relu'))\n",
        "# MaxPooling2D\n",
        "model.add(MaxPooling2D((2, 2),strides=(2,2)))\n",
        "\n",
        "# 차원 내리기용 Flatten 과정\n",
        "model.add(Flatten())\n",
        "# fully-connected layer \n",
        "model.add(Dense(512, activation='relu'))\n",
        "# Overfitting 방지를 위한 Dropout 투입\n",
        "model.add(Dropout(0.5))\n",
        "# fully-connected layer \n",
        "model.add(Dense(256, activation='relu'))\n",
        "# Overfitting 방지를 위한 Dropout 투입\n",
        "model.add(Dropout(0.5))\n",
        "# fully-connected layer \n",
        "model.add(Dense(64, activation='relu'))\n",
        "# Overfitting 방지를 위한 Dropout 투입\n",
        "model.add(Dropout(0.5))\n",
        "# Output layer: fully-connected layer \n",
        "model.add(Dense(30, activation='softmax'))\n",
        "\n",
        "# 모델 컴파일\n",
        "# optimizer: 모델을 업데이트 하는 방식\n",
        "# loss: 모델의 정확도를 판단하는 방식\n",
        "# metrics: 트레이닝 및 테스팅 성능 모니터링을 위한 평가지표\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "# 체크포인트 생성\n",
        "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath='/content/checkpoint_entire_best.h5', monitor='val_accuracy', verbose=1, save_weight_only=False, save_best_only=True, mode='auto')\n",
        "\n",
        "# 모델 트레이닝\n",
        "# batch_size: 전체 데이터셋 중 몇개씩 학습시킬 것인지\n",
        "# epoch: 학습에 전체 데이터셋이 총 몇번 이용될 것인지\n",
        "# shuffle: 학습전에 트레이닝 데이터셋을 랜덤하게 섞을 것인지\n",
        "# validation_data: 중간 성능 검증에 사용할 data set (x_test1_after, x_test2_after, 혹은 둘을 merge해서 사용)\n",
        "model.fit(x_train_after, y_train, batch_size = 128, epochs = 20, shuffle=True, callbacks=[cp_callback], validation_data=(x_test_after, y_test))\n",
        "\n",
        "# 실제로 Epoch을 100까지 했는데 아래의 Epoch 데이터가 날라고 .h5 파일만 남아서 20 Epoch으로 다시 돌린 것을 올립니다\n",
        "# Epoch 100으로 하면 val_accuracy가 0.91보다 더 높게 나옵니다\n",
        "\n",
        "model.summary()\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.7634 - accuracy: 0.7710\n",
            "Epoch 1: val_accuracy improved from -inf to 0.86455, saving model to /content/checkpoint_entire_best.h5\n",
            "1407/1407 [==============================] - 23s 16ms/step - loss: 0.7633 - accuracy: 0.7711 - val_loss: 0.5494 - val_accuracy: 0.8645\n",
            "Epoch 2/20\n",
            "1407/1407 [==============================] - ETA: 0s - loss: 0.2878 - accuracy: 0.9117\n",
            "Epoch 2: val_accuracy improved from 0.86455 to 0.88985, saving model to /content/checkpoint_entire_best.h5\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.2878 - accuracy: 0.9117 - val_loss: 0.4350 - val_accuracy: 0.8899\n",
            "Epoch 3/20\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.2290 - accuracy: 0.9316\n",
            "Epoch 3: val_accuracy did not improve from 0.88985\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 0.2289 - accuracy: 0.9316 - val_loss: 0.6076 - val_accuracy: 0.8853\n",
            "Epoch 4/20\n",
            "1403/1407 [============================>.] - ETA: 0s - loss: 0.1944 - accuracy: 0.9416\n",
            "Epoch 4: val_accuracy improved from 0.88985 to 0.89882, saving model to /content/checkpoint_entire_best.h5\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 0.1944 - accuracy: 0.9416 - val_loss: 0.4458 - val_accuracy: 0.8988\n",
            "Epoch 5/20\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.1717 - accuracy: 0.9485\n",
            "Epoch 5: val_accuracy improved from 0.89882 to 0.89903, saving model to /content/checkpoint_entire_best.h5\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 0.1717 - accuracy: 0.9485 - val_loss: 0.4848 - val_accuracy: 0.8990\n",
            "Epoch 6/20\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.1600 - accuracy: 0.9518\n",
            "Epoch 6: val_accuracy did not improve from 0.89903\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 0.1602 - accuracy: 0.9518 - val_loss: 0.6426 - val_accuracy: 0.8763\n",
            "Epoch 7/20\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.1468 - accuracy: 0.9558\n",
            "Epoch 7: val_accuracy improved from 0.89903 to 0.90737, saving model to /content/checkpoint_entire_best.h5\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.1469 - accuracy: 0.9558 - val_loss: 0.4441 - val_accuracy: 0.9074\n",
            "Epoch 8/20\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.1381 - accuracy: 0.9583\n",
            "Epoch 8: val_accuracy did not improve from 0.90737\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 0.1381 - accuracy: 0.9583 - val_loss: 0.5826 - val_accuracy: 0.9007\n",
            "Epoch 9/20\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.1288 - accuracy: 0.9606\n",
            "Epoch 9: val_accuracy did not improve from 0.90737\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.1288 - accuracy: 0.9606 - val_loss: 0.6649 - val_accuracy: 0.8978\n",
            "Epoch 10/20\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.1247 - accuracy: 0.9621\n",
            "Epoch 10: val_accuracy did not improve from 0.90737\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.1247 - accuracy: 0.9621 - val_loss: 0.7318 - val_accuracy: 0.8960\n",
            "Epoch 11/20\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.1170 - accuracy: 0.9641\n",
            "Epoch 11: val_accuracy did not improve from 0.90737\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.1170 - accuracy: 0.9641 - val_loss: 0.8226 - val_accuracy: 0.8833\n",
            "Epoch 12/20\n",
            "1403/1407 [============================>.] - ETA: 0s - loss: 0.1118 - accuracy: 0.9658\n",
            "Epoch 12: val_accuracy improved from 0.90737 to 0.91945, saving model to /content/checkpoint_entire_best.h5\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.1118 - accuracy: 0.9658 - val_loss: 0.4460 - val_accuracy: 0.9194\n",
            "Epoch 13/20\n",
            "1403/1407 [============================>.] - ETA: 0s - loss: 0.1087 - accuracy: 0.9669\n",
            "Epoch 13: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 21s 15ms/step - loss: 0.1087 - accuracy: 0.9669 - val_loss: 0.5002 - val_accuracy: 0.9110\n",
            "Epoch 14/20\n",
            "1404/1407 [============================>.] - ETA: 0s - loss: 0.1037 - accuracy: 0.9684\n",
            "Epoch 14: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 0.1036 - accuracy: 0.9684 - val_loss: 0.8523 - val_accuracy: 0.8937\n",
            "Epoch 15/20\n",
            "1403/1407 [============================>.] - ETA: 0s - loss: 0.0998 - accuracy: 0.9697\n",
            "Epoch 15: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 0.0997 - accuracy: 0.9697 - val_loss: 0.7111 - val_accuracy: 0.9036\n",
            "Epoch 16/20\n",
            "1403/1407 [============================>.] - ETA: 0s - loss: 0.0950 - accuracy: 0.9704\n",
            "Epoch 16: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 20s 15ms/step - loss: 0.0951 - accuracy: 0.9704 - val_loss: 0.7546 - val_accuracy: 0.8992\n",
            "Epoch 17/20\n",
            "1405/1407 [============================>.] - ETA: 0s - loss: 0.0970 - accuracy: 0.9704\n",
            "Epoch 17: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.0970 - accuracy: 0.9704 - val_loss: 0.7581 - val_accuracy: 0.9007\n",
            "Epoch 18/20\n",
            "1404/1407 [============================>.] - ETA: 0s - loss: 0.0913 - accuracy: 0.9712\n",
            "Epoch 18: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 0.0914 - accuracy: 0.9712 - val_loss: 0.7880 - val_accuracy: 0.8985\n",
            "Epoch 19/20\n",
            "1406/1407 [============================>.] - ETA: 0s - loss: 0.0906 - accuracy: 0.9719\n",
            "Epoch 19: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 20s 14ms/step - loss: 0.0905 - accuracy: 0.9719 - val_loss: 0.7950 - val_accuracy: 0.9039\n",
            "Epoch 20/20\n",
            "1403/1407 [============================>.] - ETA: 0s - loss: 0.0907 - accuracy: 0.9726\n",
            "Epoch 20: val_accuracy did not improve from 0.91945\n",
            "1407/1407 [==============================] - 19s 14ms/step - loss: 0.0907 - accuracy: 0.9726 - val_loss: 1.5226 - val_accuracy: 0.8823\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_44 (Conv2D)          (None, 25, 25, 3)         51        \n",
            "                                                                 \n",
            " zero_padding2d_10 (ZeroPadd  (None, 27, 27, 3)        0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_45 (Conv2D)          (None, 24, 24, 64)        3136      \n",
            "                                                                 \n",
            " max_pooling2d_16 (MaxPoolin  (None, 12, 12, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " zero_padding2d_11 (ZeroPadd  (None, 14, 14, 64)       0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_46 (Conv2D)          (None, 11, 11, 128)       131200    \n",
            "                                                                 \n",
            " zero_padding2d_12 (ZeroPadd  (None, 13, 13, 128)      0         \n",
            " ing2D)                                                          \n",
            "                                                                 \n",
            " conv2d_47 (Conv2D)          (None, 10, 10, 128)       262272    \n",
            "                                                                 \n",
            " max_pooling2d_17 (MaxPoolin  (None, 5, 5, 128)        0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 3200)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 512)               1638912   \n",
            "                                                                 \n",
            " dropout_31 (Dropout)        (None, 512)               0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 256)               131328    \n",
            "                                                                 \n",
            " dropout_32 (Dropout)        (None, 256)               0         \n",
            "                                                                 \n",
            " dense_38 (Dense)            (None, 64)                16448     \n",
            "                                                                 \n",
            " dropout_33 (Dropout)        (None, 64)                0         \n",
            "                                                                 \n",
            " dense_39 (Dense)            (None, 30)                1950      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,185,297\n",
            "Trainable params: 2,185,297\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "CKztKsFQjMfR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QR9WUYXxqtfR"
      },
      "source": [
        "# **4. 모델 저장**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wi9yznz4qvzK"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team08'\n",
        "\n",
        "# 트레이닝된 전체 모델을 저장합니다.\n",
        "model.save(save_path +  'model_entire_'+ team_name + '.h5')"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4aPbgI-c-Kj8"
      },
      "source": [
        "# **5. 모델 로드 및 평가**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7WONVxH-Kt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdb18e71-93d4-426b-c921-796fa42ca647"
      },
      "source": [
        "save_path = '/content/'\n",
        "team_name = 'team08'\n",
        "\n",
        "model = keras.models.load_model(save_path + 'model_entire_' + team_name + '.h5')\n",
        "\n",
        "model.evaluate(x_test_after, y_test)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1250/1250 [==============================] - 13s 3ms/step - loss: 0.6649 - accuracy: 0.9138\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.6648905277252197, 0.9138000011444092]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jHouRUVdaGzE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}